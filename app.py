# =========================
# config
# =========================
# ç’°å¢ƒãƒ»åŸºæœ¬è¨­å®šï¼ˆ.env ã‹ã‚‰ OPENAI_API_KEY ã‚’èª­ã¿è¾¼ã‚€ï¼‰
import os
import re
from dataclasses import dataclass
from typing import Optional, List

from dotenv import load_dotenv  # .env èª­ã¿è¾¼ã¿
load_dotenv()

import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import SystemMessage, HumanMessage
from langchain_community.vectorstores import Chroma


# åŸºæœ¬è¨­å®šã‚’ã²ã¨ã¾ã¨ã‚ã«ã™ã‚‹
@dataclass
class Config:
    app_name: str = "streamlit-llm-app"            # ã‚¢ãƒ—ãƒªåï¼ˆDBãƒ‘ã‚¹ã®ã‚¹ãƒ©ã‚°åŒ–ã«ä½¿ç”¨ï¼‰
    model_name: str = "gpt-4o-mini"                # LLMãƒ¢ãƒ‡ãƒ«
    temperature: float = 0.0                       # å‡ºåŠ›ã®å¤šæ§˜æ€§
    embed_model: str = "text-embedding-3-small"    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
    top_k: int = 3                                 # RAGã®å–å¾—ä»¶æ•°


# =========================
# utils
# =========================
def slugify_ascii(text: str) -> str:
    """ASCII/slugåŒ–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç”¨ã«å®‰å…¨åŒ–ï¼‰"""
    text = text.lower()
    text = re.sub(r"[^a-z0-9]+", "-", text)
    return re.sub(r"-+", "-", text).strip("-")


def build_llm(cfg: Config) -> ChatOpenAI:
    """LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆï¼ˆä¾å­˜æ³¨å…¥ã®ãŸã‚åˆ†é›¢ï¼‰"""
    return ChatOpenAI(model_name=cfg.model_name, temperature=cfg.temperature)


# =========================
# stores
# =========================
def build_vectorstore(cfg: Config) -> Chroma:
    """Chroma(0.4+)ã§è»½é‡ã®æ–¹é‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¼ç´ï¼ˆpersist()ä¸è¦ï¼‰"""
    # personaæ–¹é‡ï¼ˆæœ€å°ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ï¼‰
    texts = [
        "persona:A æ–¹é‡: ã‚ãªãŸã¯æ—¥æœ¬ã®åŠ´å‹™ãƒ»å°±æ¥­è¦å‰‡ã®ä¸€èˆ¬çš„ãªåŠ©è¨€è€…ã€‚æ³•ä»¤ã®æœ€çµ‚åˆ¤æ–­ã¯ä¸€æ¬¡æƒ…å ±ã‚„å°‚é–€å®¶ç¢ºèªã‚’ä¿ƒã™ã€‚å€‹åˆ¥ã®æ³•çš„åŠ©è¨€ã¯é¿ã‘ã€å®Ÿå‹™ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚„ç›¸è«‡å…ˆã‚’ææ¡ˆã™ã‚‹ã€‚",
        "persona:B æ–¹é‡: ã‚ãªãŸã¯Python/ç”ŸæˆAIã®æ¥­å‹™æ”¹å–„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€‚å°ã•ãä½œã£ã¦æ¤œè¨¼ã€ãƒ­ã‚°ã¨å†ç¾æ€§ã‚’é‡è¦–ã—ã€APIã‚­ãƒ¼ç®¡ç†ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã«é…æ…®ã—ãŸæ‰‹é †ã‚’ç°¡æ½”ã«ç¤ºã™ã€‚",
    ]
    metadatas = [{"persona": "A"}, {"persona": "B"}]

    embeddings = OpenAIEmbeddings(model=cfg.embed_model)
    persist_dir = f".chroma-{slugify_ascii(cfg.app_name)}"  # DBãƒ‘ã‚¹ã¯ASCII/slugåŒ–
    # ã“ã“ã§ã¯ç°¡ä¾¿ã«æ¯å› from_texts ã§ä½œæˆï¼ˆèª²é¡Œè¦ä»¶ä¸ŠOKï¼‰
    vs = Chroma.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas, persist_directory=persist_dir)
    return vs


def build_retriever(cfg: Config):
    """VectorStoreã‹ã‚‰Retrieverã‚’ç”Ÿæˆ"""
    vs = build_vectorstore(cfg)
    return vs.as_retriever(search_kwargs={"k": cfg.top_k})


# =========================
# indexing
# =========================
def retrieve_context(query: str, persona: str, retriever) -> List[str]:
    """ã‚¯ã‚¨ãƒªï¼‹ãƒšãƒ«ã‚½ãƒŠã§é–¢é€£æ–‡è„ˆã‚’å–å¾—ï¼ˆå¤±æ•—æ™‚ã¯ç©ºãƒªã‚¹ãƒˆï¼‰"""
    try:
        docs = retriever.get_relevant_documents(query)
        selected = [d.page_content for d in docs if d.metadata.get("persona") == persona]
        return selected or [d.page_content for d in docs]
    except Exception:
        return []


# =========================
# tools
# =========================
def make_system_prompt(persona: str, contexts: Optional[List[str]] = None) -> str:
    """ãƒšãƒ«ã‚½ãƒŠåˆ¥ã®Systemãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆã—ã€è£œåŠ©æ–‡è„ˆã‚’ä»˜åŠ """
    persona_map = {
        "A": (
            "You are a Japanese labor/HR compliance advisor. "
            "Provide general, practical, and safe guidance. "
            "For legal decisions, encourage users to check official sources or consult professionals."
        ),
        "B": (
            "You are a Python & Generative AI productivity engineer. "
            "Propose small testable steps, concise examples, and consider security & reproducibility."
        ),
    }
    base = persona_map.get(persona, "You are a helpful assistant.")
    ctx = ""
    if contexts:
        ctx = "\n\n# Additional context (persona policy)\n" + "\n".join(f"- {c}" for c in contexts)
    return base + ctx


# =========================
# agent
# =========================
def generate_response(
    user_text: str,
    persona_value: str,
    llm: ChatOpenAI,
    retriever=None,
) -> str:
    """å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ã‚¸ã‚ªé¸æŠã‚’å—ã‘å–ã‚Šã€LLMã®å›ç­”æ–‡å­—åˆ—ã‚’è¿”ã™"""
    try:
        if not user_text.strip():
            return "å…¥åŠ›ãŒç©ºã§ã™ã€‚ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"

        persona = "A" if persona_value.startswith("A") else "B"
        contexts = retrieve_context(user_text, persona, retriever) if retriever else []
        system_prompt = make_system_prompt(persona, contexts)

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_text),
        ]
        result = llm.invoke(messages)  # LangChain v0.2+ ã®æ¨å¥¨å‘¼ã³å‡ºã—
        return result.content or "ï¼ˆLLMã‹ã‚‰ç©ºã®å¿œç­”ãŒè¿”ã‚Šã¾ã—ãŸï¼‰"

    except Exception as e:
        msg = str(e)
        if "OPENAI_API_KEY" in msg or "api key" in msg.lower():
            return "OpenAI APIã‚­ãƒ¼ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚.env ã® OPENAI_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        if "rate limit" in msg.lower():
            return "ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        if "timeout" in msg.lower():
            return "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ¥ç¶šç’°å¢ƒã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚åŸå› ã®æ¨æ¸¬: {msg}"


# =========================
# app (Streamlit)
# =========================
def run_app():
    """Streamlit UI æœ¬ä½“"""
    st.set_page_config(page_title="LLMã‚¢ãƒ—ãƒªï¼ˆLangChain + Streamlitï¼‰", page_icon="ğŸ¤–", layout="centered")

    st.title("LLMã‚¢ãƒ—ãƒªï¼ˆLangChain + Streamlitï¼‰")
    st.caption("å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«ç›¸è«‡å†…å®¹ã‚’è¨˜å…¥ã—ã€å°‚é–€å®¶A/Bã‚’é¸ã‚“ã§é€ä¿¡ã—ã¦ãã ã•ã„ã€‚LangChainçµŒç”±ã§LLMãŒå›ç­”ã—ã¾ã™ã€‚")

    with st.expander("ã“ã®ã‚¢ãƒ—ãƒªã®æ¦‚è¦ / æ“ä½œæ–¹æ³•", expanded=True):
        st.markdown(
            "- å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã¯1ã¤ã§ã™ã€‚\n"
            "- ãƒ©ã‚¸ã‚ªã§ **å°‚é–€å®¶ã‚¿ã‚¤ãƒ—** ã‚’é¸æŠï¼ˆA: åŠ´å‹™ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ / B: æ¥­å‹™æ”¹å–„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼‰ã€‚\n"
            "- é¸æŠã«å¿œã˜ã¦ System ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ‡ã‚Šæ›¿ãˆã€LLMã«æŠ•ã’ã¾ã™ã€‚\n"
            "- APIã‚­ãƒ¼ã¯ `.env` ã® `OPENAI_API_KEY` ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆGitHubã¸ã¯å«ã‚ãªã„ï¼‰ã€‚"
        )

    if not os.getenv("OPENAI_API_KEY"):
        st.error("OPENAI_API_KEY ãŒç’°å¢ƒå¤‰æ•°ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    cfg = Config()
    try:
        llm = build_llm(cfg)  # LLMåˆæœŸåŒ–
    except Exception as e:
        st.error(f"LLMåˆæœŸåŒ–ã§ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    retriever = None
    try:
        retriever = build_retriever(cfg)  # è»½é‡RAG
    except Exception as e:
        st.warning(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆRAGãªã—ã§ç¶™ç¶šï¼‰: {e}")

    persona = st.radio(
        "å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠ",
        options=["Aï½œåŠ´å‹™ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼", "Bï½œæ¥­å‹™æ”¹å–„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢"],
        horizontal=True,
    )
    user_text = st.text_area("ç›¸è«‡å†…å®¹ï¼ˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ï¼‰", height=160, placeholder="ä¾‹ï¼š36å”å®šã®é‹ç”¨ãƒã‚¤ãƒ³ãƒˆ / å°ã•ãªè‡ªå‹•åŒ–ã®å§‹ã‚æ–¹")
    if st.button("é€ä¿¡"):
        with st.spinner("LLMã«å•ã„åˆã‚ã›ä¸­..."):
            answer = generate_response(user_text, persona, llm, retriever)
        st.markdown("### å›ç­”")
        st.write(answer)


# =========================
# mainï¼ˆStreamlitã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆï¼‰
# =========================
if __name__ == "__main__":
    run_app()  # Streamlitã§èµ·å‹•ã—ãŸéš›ã«UIã‚’æç”»
